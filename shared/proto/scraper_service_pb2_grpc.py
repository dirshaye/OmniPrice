# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
import grpc

from shared.proto import scraper_service_pb2 as shared_dot_proto_dot_scraper__service__pb2


class ScraperServiceStub(object):
    """Scraper Service Definition
    """

    def __init__(self, channel):
        """Constructor.

        Args:
            channel: A grpc.Channel.
        """
        self.StartScrapeJob = channel.unary_unary(
                '/omnipricex.scraper.ScraperService/StartScrapeJob',
                request_serializer=shared_dot_proto_dot_scraper__service__pb2.StartScrapeJobRequest.SerializeToString,
                response_deserializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.FromString,
                )
        self.GetScrapeJobStatus = channel.unary_unary(
                '/omnipricex.scraper.ScraperService/GetScrapeJobStatus',
                request_serializer=shared_dot_proto_dot_scraper__service__pb2.GetScrapeJobStatusRequest.SerializeToString,
                response_deserializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.FromString,
                )
        self.CancelScrapeJob = channel.unary_unary(
                '/omnipricex.scraper.ScraperService/CancelScrapeJob',
                request_serializer=shared_dot_proto_dot_scraper__service__pb2.CancelScrapeJobRequest.SerializeToString,
                response_deserializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.FromString,
                )
        self.ListScrapeJobs = channel.unary_unary(
                '/omnipricex.scraper.ScraperService/ListScrapeJobs',
                request_serializer=shared_dot_proto_dot_scraper__service__pb2.ListScrapeJobsRequest.SerializeToString,
                response_deserializer=shared_dot_proto_dot_scraper__service__pb2.ListScrapeJobsResponse.FromString,
                )
        self.GetScrapedData = channel.unary_unary(
                '/omnipricex.scraper.ScraperService/GetScrapedData',
                request_serializer=shared_dot_proto_dot_scraper__service__pb2.GetScrapedDataRequest.SerializeToString,
                response_deserializer=shared_dot_proto_dot_scraper__service__pb2.GetScrapedDataResponse.FromString,
                )
        self.ScrapeUrls = channel.unary_unary(
                '/omnipricex.scraper.ScraperService/ScrapeUrls',
                request_serializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeUrlsRequest.SerializeToString,
                response_deserializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.FromString,
                )


class ScraperServiceServicer(object):
    """Scraper Service Definition
    """

    def StartScrapeJob(self, request, context):
        """Start a new scrape job
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def GetScrapeJobStatus(self, request, context):
        """Get the status of a scrape job
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def CancelScrapeJob(self, request, context):
        """Cancel a running scrape job
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def ListScrapeJobs(self, request, context):
        """List scrape jobs
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def GetScrapedData(self, request, context):
        """Get scraped data for a job
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def ScrapeUrls(self, request, context):
        """Manually trigger a scrape for specific URLs
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_ScraperServiceServicer_to_server(servicer, server):
    rpc_method_handlers = {
            'StartScrapeJob': grpc.unary_unary_rpc_method_handler(
                    servicer.StartScrapeJob,
                    request_deserializer=shared_dot_proto_dot_scraper__service__pb2.StartScrapeJobRequest.FromString,
                    response_serializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.SerializeToString,
            ),
            'GetScrapeJobStatus': grpc.unary_unary_rpc_method_handler(
                    servicer.GetScrapeJobStatus,
                    request_deserializer=shared_dot_proto_dot_scraper__service__pb2.GetScrapeJobStatusRequest.FromString,
                    response_serializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.SerializeToString,
            ),
            'CancelScrapeJob': grpc.unary_unary_rpc_method_handler(
                    servicer.CancelScrapeJob,
                    request_deserializer=shared_dot_proto_dot_scraper__service__pb2.CancelScrapeJobRequest.FromString,
                    response_serializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.SerializeToString,
            ),
            'ListScrapeJobs': grpc.unary_unary_rpc_method_handler(
                    servicer.ListScrapeJobs,
                    request_deserializer=shared_dot_proto_dot_scraper__service__pb2.ListScrapeJobsRequest.FromString,
                    response_serializer=shared_dot_proto_dot_scraper__service__pb2.ListScrapeJobsResponse.SerializeToString,
            ),
            'GetScrapedData': grpc.unary_unary_rpc_method_handler(
                    servicer.GetScrapedData,
                    request_deserializer=shared_dot_proto_dot_scraper__service__pb2.GetScrapedDataRequest.FromString,
                    response_serializer=shared_dot_proto_dot_scraper__service__pb2.GetScrapedDataResponse.SerializeToString,
            ),
            'ScrapeUrls': grpc.unary_unary_rpc_method_handler(
                    servicer.ScrapeUrls,
                    request_deserializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeUrlsRequest.FromString,
                    response_serializer=shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.SerializeToString,
            ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
            'omnipricex.scraper.ScraperService', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler,))


 # This class is part of an EXPERIMENTAL API.
class ScraperService(object):
    """Scraper Service Definition
    """

    @staticmethod
    def StartScrapeJob(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/omnipricex.scraper.ScraperService/StartScrapeJob',
            shared_dot_proto_dot_scraper__service__pb2.StartScrapeJobRequest.SerializeToString,
            shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def GetScrapeJobStatus(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/omnipricex.scraper.ScraperService/GetScrapeJobStatus',
            shared_dot_proto_dot_scraper__service__pb2.GetScrapeJobStatusRequest.SerializeToString,
            shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def CancelScrapeJob(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/omnipricex.scraper.ScraperService/CancelScrapeJob',
            shared_dot_proto_dot_scraper__service__pb2.CancelScrapeJobRequest.SerializeToString,
            shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def ListScrapeJobs(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/omnipricex.scraper.ScraperService/ListScrapeJobs',
            shared_dot_proto_dot_scraper__service__pb2.ListScrapeJobsRequest.SerializeToString,
            shared_dot_proto_dot_scraper__service__pb2.ListScrapeJobsResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def GetScrapedData(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/omnipricex.scraper.ScraperService/GetScrapedData',
            shared_dot_proto_dot_scraper__service__pb2.GetScrapedDataRequest.SerializeToString,
            shared_dot_proto_dot_scraper__service__pb2.GetScrapedDataResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def ScrapeUrls(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/omnipricex.scraper.ScraperService/ScrapeUrls',
            shared_dot_proto_dot_scraper__service__pb2.ScrapeUrlsRequest.SerializeToString,
            shared_dot_proto_dot_scraper__service__pb2.ScrapeJobStatusResponse.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
